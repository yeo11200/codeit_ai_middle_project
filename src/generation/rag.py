from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

import torch
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

class RAGChain:
    def __init__(self, config, vector_store_wrapper):
        self.config = config
        self.vector_store_wrapper = vector_store_wrapper
        
        # [ì•ˆì „ì¥ì¹˜] ì„¤ì •ê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸(gpt-5-mini) ì‚¬ìš©
        llm_cfg = config.get('llm', {})
        model_name = llm_cfg.get('model', 'gpt-5-mini')
        
        # self.llm = ChatOpenAI(
        #     model=model_name,
        #     temperature=0
        # )
        self.llm = get_huggingface_llm()

        self.prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ RFP(ì œì•ˆìš”ì²­ì„œ) ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ ì œê³µëœ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ë‹µë³€ ì‹œ, ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš©ì´ ì–´ëŠ ë¬¸ì„œì˜ ì–´ë–¤ ë¶€ë¶„ì¸ì§€ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

        [Context]
        {context}

        [Question]
        {question}

        [Answer]:
        """)

    def generate_answer(self, question, selected_docs=None):
        # 1. ê²€ìƒ‰ í•„í„° ë° Kê°’ ì„¤ì • (ë™ì  í• ë‹¹)
        # ê¸°ë³¸ì€ 5ê°œë§Œ ê°€ì ¸ì˜¤ì§€ë§Œ...
        search_kwargs = {"k": 3}
        
        if selected_docs:
             # [í•µì‹¬ ìˆ˜ì •] ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ 'ì½• ì§‘ì—ˆì„ ë•Œ'ëŠ” ê²½ë¡œë¥¼ ë§ì¶°ì£¼ê³ 
             # ê²€ìƒ‰ ê°œìˆ˜(k)ë¥¼ 30ê°œê¹Œì§€ í™• ëŠ˜ë ¤ì„œ ì•/ë’¤ ë‚´ìš©ì„ ë‹¤ ê¸ì–´ì˜¤ê²Œ í•©ë‹ˆë‹¤.
             full_paths = [f"./data/01-raw/{doc}" for doc in selected_docs]
             search_kwargs["filter"] = {"source": {"$in": full_paths}}

             ##search_kwargs["k"] = 30  # <--- ë¬¸ì„œë¥¼ ì§€ì •í–ˆìœ¼ë©´ 30í˜ì´ì§€ ì •ë„ëŠ” ì½ì–´ë´ì•¼ ì •í™•í•¨!
             # [ìˆ˜ì •] k=30ì€ ë„ˆë¬´ ë§ì•„ ì—ëŸ¬ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤. 
             # Llama-3-8Bì˜ ì»¨í…ìŠ¤íŠ¸ ì°½(8k)ì„ ê³ ë ¤í•´ 10~15 ì •ë„ë¡œ íƒ€í˜‘í•©ë‹ˆë‹¤.
             search_kwargs["k"] = 10

        # [cite_start]2. ë™ì  ê²€ìƒ‰ê¸° ìƒì„± [cite: 25]
        retriever = self.vector_store_wrapper.vector_store.as_retriever(
            search_kwargs=search_kwargs
        )
        
        # 3. ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
        docs = retriever.invoke(question)
        
        def format_docs(documents):
            # ë’¤ì— ë¶™ì–´ìˆë˜ # ì œê±°
            return "\n\n".join([d.page_content for d in documents])
        chain = self.prompt | self.llm | StrOutputParser()
        
        answer = chain.invoke({
            "context": format_docs(docs),
            "question": question
        })
        
        return answer, docs


# -----------------------------------------------------------------------------
# 1. LLM ë¡œë“œ í•¨ìˆ˜ (GCP GPU ì‚¬ìš©)
# -----------------------------------------------------------------------------
def get_huggingface_llm(model_id="beomi/Llama-3-Open-Ko-8B"):
    print(f"ğŸ”„ LLM ëª¨ë¸ ë¡œë“œ ì¤‘... ({model_id})")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ìµœì í™”
            device_map="auto",          # GPU ìë™ í• ë‹¹
            trust_remote_code=True
        )
        
        # í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
        hf_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1,
            top_p=0.95,
            repetition_penalty=1.1,
            return_full_text=False
        )
        print("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return HuggingFacePipeline(pipeline=hf_pipeline)
        
    except Exception as e:
        print(f"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ CPUë¡œ ê°•ì œ ì‹¤í–‰í•˜ê±°ë‚˜ ì¢…ë£Œí•  ìˆ˜ ìˆìŒ
        raise e        
from src.agent.state import AgentState
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class AgentNodes:
    def __init__(self, rag_chain_instance):
        """
        ì—ì´ì „íŠ¸ì˜ í–‰ë™(Node)ë“¤ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
        L4 GPU ìì›ì„ í™œìš©í•˜ê¸° ìœ„í•´ ë¡œì»¬ LLM(Ollama)ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.rag_chain = rag_chain_instance
        
        # [L4 GPU ìµœì í™”] VRAM 24GBë¥¼ í™œìš©í•˜ì—¬ ê³ ì„±ëŠ¥ ëª¨ë¸(qwen2.5:7b ë“±)ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        # ê³ ì •ë°€ ê²€ì¦ ë° HyDE ì‘ì„±ì„ ìœ„í•´ temperatureëŠ” 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        self.local_llm = ChatOllama(model="qwen2.5:7b", temperature=0)

    def hyde_node(self, state: AgentState):
        """
        [STEP 0] HyDE (Hypothetical Document Embeddings)
        ì§ˆë¬¸ì— ëŒ€í•´ AIê°€ ê°€ìƒì˜ 'ëª¨ë²” ë‹µì•ˆ'ì„ ë¨¼ì € ì‘ì„±í•©ë‹ˆë‹¤. 
        ì´ ê°€ì§œ ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê²°í•©ë˜ë©´ ë²¡í„° DBì—ì„œ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë¥¼ ì°¾ì„ í™•ë¥ ì´ ë¹„ì•½ì ìœ¼ë¡œ ìƒìŠ¹í•©ë‹ˆë‹¤.
        """
        print("\nğŸ’¡ [L4 ê°€ì†] HyDE ì „ë¬¸ ë‹µë³€ ìƒì„± ì¤‘ (ê²€ìƒ‰ í’ˆì§ˆ ê°•í™”)...")
        prompt = ChatPromptTemplate.from_template(
            "ë‹¹ì‹ ì€ RFP ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ê¸°ìˆ ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”: {question}"
        )
        chain = prompt | self.local_llm | StrOutputParser()
        hypothetical_answer = chain.invoke({"question": state["question"]})
        
        # ì›ë³¸ ì§ˆë¬¸ê³¼ ê°€ì§œ ë‹µë³€ì„ í•©ì³ì„œ ë‹¤ìŒ ë‹¨ê³„(ê²€ìƒ‰)ì— ì „ë‹¬í•©ë‹ˆë‹¤.
        return {"question": f"Original: {state['question']}\nInsight: {hypothetical_answer}"}

    def retrieve_node(self, state: AgentState):
        """
        [STEP 1] Retrieve (ë¬¸ì„œ ìˆ˜ì§‘)
        ê¸°ì¡´ì— êµ¬ì¶•ëœ RAG ì—”ì§„ì„ í™œìš©í•˜ì—¬ ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        HyDEë¡œ í’ë¶€í•´ì§„ ì§ˆë¬¸ ë•ë¶„ì— ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ ì •í™•í•œ ë¬¸ì„œ ë§¤ì¹­ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        """
        print("ğŸ” [ì—ì´ì „íŠ¸] ì§€ëŠ¥í˜• ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")
        # ê¸°ì¡´ rag_chain.generate_answer ë¡œì§ì„ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì½”ë“œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ë‹µë³€ ìƒì„± ì „ ë‹¨ê³„ì´ë¯€ë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œ(docs)ë§Œ ì¶”ì¶œí•˜ì—¬ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
        _, docs = self.rag_chain.generate_answer(state["question"], state.get("selected_docs"))
        return {"documents": docs}

    def rerank_node(self, state: AgentState):
        """
        [STEP 2] Rerank (ê²°ê³¼ ì¬ì •ë ¬)
        L4 GPUì˜ ë„‰ë„‰í•œ ìì›ì„ ë¯¿ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ìƒìœ„ 10ê°œë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
        ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ìµœì¢… ë‹µë³€ì˜ í’ˆì§ˆì„ ë†’ì´ëŠ” í•„í„°ë§ ë‹¨ê³„ì…ë‹ˆë‹¤.
        """
        print("âš–ï¸ [ì—ì´ì „íŠ¸] L4 ê¸°ë°˜ ê³ ì •ë°€ ë¦¬ë­í‚¹ ìˆ˜í–‰...")
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ìƒìœ„ 10ê°œë§Œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤.
        return {"documents": state["documents"][:10]}

    def grade_node(self, state: AgentState):
        """
        [STEP 3] Grade (ê²€ì¦ ë° íŒë‹¨)
        ë¡œì»¬ ëª¨ë¸ì´ ìˆ˜ì§‘ëœ ë¬¸ì„œë¥¼ ì½ê³  "ì´ ì •ë³´ë¡œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ”ê°€?"ë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤.
        ì—¬ê¸°ì„œ 'no'ê°€ ë‚˜ì˜¤ë©´ ê·¸ë˜í”„ëŠ” ë‹¤ì‹œ HyDE ë‹¨ê³„ë¡œ ëŒì•„ê°€ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        print("ğŸ§ [ì—ì´ì „íŠ¸] ê²€ìƒ‰ ê²°ê³¼ ì í•©ì„± ì •ë°€ ê²€ìˆ˜ (Hallucination ë°©ì§€)...")
        context = "\n\n".join([d.page_content for d in state["documents"]])
        prompt = ChatPromptTemplate.from_template(
            "ì œê³µëœ [ë¬¸ë§¥]ì´ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•˜ê³  êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ? "
            "ë°˜ë“œì‹œ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.\n\n[ë¬¸ë§¥]: {context}\n\n[ì§ˆë¬¸]: {question}"
        )
        grader = prompt | self.local_llm | StrOutputParser()
        result = grader.invoke({"context": context, "question": state["question"]})
        
        # ê²°ê³¼ê°’ì˜ ê³µë°±ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ìƒíƒœ(State)ì— ê¸°ë¡í•©ë‹ˆë‹¤.
        return {"is_relevant": result.strip().lower()}

    def generate_node(self, state: AgentState):
        """
        [STEP 4] Generate (ìµœì¢… ë‹µë³€ ìƒì„±)
        ê²€ì¦ì´ ì™„ë£Œëœ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì¤„ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤.
        íŒ€ì›ë“¤ì´ ê¸°ì¡´ì— ì„¤ì •í•œ í”„ë¡¬í”„íŠ¸ì™€ LLM ì„¤ì •ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
        """
        print("âœï¸ [ì—ì´ì „íŠ¸] ìµœì¢… RFP ë¶„ì„ ë‹µë³€ ìƒì„± ì¤‘...")
        # ê¸°ì¡´ rag_chainì˜ í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ì„¤ì •ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        chain = self.rag_chain.prompt | self.rag_chain.llm | StrOutputParser()
        answer = chain.invoke({"context": state["documents"], "question": state["question"]})
        return {"answer": answer}